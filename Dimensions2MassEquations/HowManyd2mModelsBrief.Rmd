---
title: "Impacts of not sampling d2m every year in every refuge"
author: "Emilio A. Laca"
date: "11/10/2021"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE, echo=FALSE}

knitr::opts_chunk$set(echo = TRUE)
library(kableExtra)
library(tidyverse)
library(car)
library(magrittr)
library(readxl)
library(emmeans)
library(lme4)
library(lmerTest)
library(rsample)
library(recipes)
library(tidymodels)
library(LMERConvenienceFunctions)
library(multilevelmod)
library(knitr)
library(ggpubr)

```

## Introduction

WHAP measures linear dimensions of seed heads in the field but attempts to estimate mass of seedheads produced. Dimensions-to-mass (d2m) models are developed and used to transform linear dimensions to estimated mass of seed heads. Then, number of seed heads per unit area and average seed head dimensions are used to estimate mass of seed heads per unit area in each of the abundance strata (low, medium and high) for each species.

Is it necessary to create a different d2m model for each location and year or is it sufficient to create one model that is applicable over refuges and years? Of course, this question can only be answered for past years and observed locations. One can assume that differences (or lack of differences) persist into the future and decide this once and for all or one can continue to test over time. This document is about exploring the answer to the question using data about seed head dimensions and mass collected in several refuges in 2019 and in Modoc in 2021.

## Data

```{r data, echo=FALSE}

st_2019 <- readRDS("st_all_noO.rds") %>%
  rename_with(~gsub(".", "_", .x, fixed = TRUE)) %>%
  mutate(year = 2019,
         spp = "SwampTimothy",
         f2t_length_mm = NA,
         length_mm = avg_length_mm,
         width_mm = NA,
         mass_mg = mass_per_sh_mg,
         LIT = substr(id, 1, 3)) %>%
  dplyr::select(LIT,
                year,
                spp,
                f2t_length_mm,
                length_mm,
                width_mm,
                mass_mg)

wg_2019 <- readRDS("wgdat.rds") %>%
  rename_with(~gsub(".", "_", .x, fixed = TRUE)) %>%
  mutate(year = 2019,
         spp = "Watergrass",
         f2t_length_mm = F2T_cm * 10,
         length_mm = sh_len_cm * 10,
         width_mm = NA,
         mass_mg = mass_per_sh_mg,
         LIT = refuge) %>%
  dplyr::select(LIT,
                year,
                spp,
                f2t_length_mm,
                length_mm,
                width_mm,
                mass_mg)

mdc_2021 <- readRDS("sh_dim_MDC2021.rds") %>%
  mutate(year = 2021) %>%
  dplyr::select(LIT,
                year,
                spp,
                f2t_length_mm,
                length_mm,
                width_mm,
                mass_mg)

d2m_data <- bind_rows(st_2019, wg_2019, mdc_2021)

unique(d2m_data$LIT)

# Fix LIT codes
LitKey <- c(CLS = "CLS",
            KRN = "KRN",
            PXL = "PIX",
            SAC = "SAC",
            STL = "SLW",
            PIX = "PIX",
            SL_ = "SLW",
            Colusa = "CLS",
            Sacramento = "SAC",
            StoneLakes = "SLW",
            MDC = "MDC")

d2m_data$LIT <- dplyr::recode(d2m_data$LIT, !!!LitKey)

# Create a grouping variable for LIT and year
d2m_data %<>% mutate(LIT_yr = paste(LIT, year, sep = "_"),
                     log_mass_mg = log(mass_mg),
                     log_length_mm = log(length_mm),
                     log_f2t_length_mm = log(f2t_length_mm + 120))


wg_d2m <- d2m_data %>%
  dplyr::filter(spp == "Watergrass") %>%
  mutate(emerged = f2t_length_mm / length_mm,
         log_emerged = log(emerged + 0.5)) %>%
  arrange(LIT, year) %>%
  dplyr::filter(!is.nan(log_emerged))

st_d2m <- d2m_data %>%
  dplyr::filter(spp == "SwampTimothy") %>%
  arrange(LIT, year)

```

## Approach and functions

We have data for Swamp Timothy in six, and for Watergrass in four, different combinations of locations and years. No refuge was sampled more than once to create d2m models. Thus, the variation among groups is mostly spatial. There is no data to estimate the possible variation within a refuge among years.

We should keep in mind that the uncertainty due to these models is NOT incorporated in the final mass estimates in the current protocol. Thus, the effect of choice of approach on precision is mostly invisible.

With these data we have the following options:

1. Collect d2m data for each new LIT_yr group. Augment the data base and create a model that always includes effects of all LIT_yr groups ever observed. Always make group-specific predictions whose uncertainty excludes the variance of random effects.

The performance of this option is assessed with the present data by k-fold cross-validation with folds balanced by groups.

2. Take all d2m data and create a model that includes effects of LIT_yr groups. No more data about d2m are collected and this model is not updated. Except for the LIT_yr groups present in the data, predictions are made at the population level and random effect variances contribute to the uncertainty of estimates.

The performance of this option is assessed with the present data by leaving each group out at a time, developing the model with the rest of the groups and using it in the group left out.

Three metrics are used to assess the performance of the options:

1. Means absolute deviation (mae) is a measure of accuracy and error.
2. Root mean squared error (rmse) is a measure of accuracy and total error.
3. R squared (rsq) is the square of the correlation between observed and predicted values and it is a measure of consistency.

Predictions that deviate from observed values in a linear relationship can have very large rsq (good) and also large rsme (bad). If the slope of the relationship is null the bias is constant over X. If the slope is different from 0, the bias changes as X changes.


### Function to evaluate options

```{r evalFun, echo=FALSE}
# Model option mld.opt has to be either "fixed" or anything else. Anything else
# results in the "mixed" model.
model_eval_fnc <- function(the_data = NULL) { # Start f() definition ******
  
  # Make vector of LIT_yr groups to loop over
  grps <- unique(the_data$LIT_yr)
  
  # Create data frames to receive output from validation (test)
  test_metrics <- tibble(grp_out = character(),
                         .metric =  character(),
                         .estimate = double())
  
  test_data4graphs <- tibble(LIT_yr = character(),
                             mass_mg =  double(),
                             .pred = double())
  
  # Determine what species is being analyzed
  spp <- the_data %>%
    pluck("spp") %>%
    unique()
  
  #Prepare data for manual initial split by LIT_yr
  the_data %<>%
    arrange(LIT_yr) %>%
    mutate(.row = row_number())
  
  # Define model
  lm_model <-
    linear_reg() %>%
    set_engine("lmer")
  
  
  # Define formulae
  frmls <- list(SwampTimothy = log_mass_mg ~
                  log_length_mm +
                  (1 | LIT_yr),
                Watergrass = log_mass_mg ~
                  log_length_mm +
                  log_f2t_length_mm +
                  (0 + log_f2t_length_mm | LIT_yr) +
                  log_emerged +
                  (0 + log_emerged | LIT_yr))
  
  
  m_formula <- frmls[[spp]]
  
  # Start of cross-validation to assess option 1
  df_vfold <- vfold_cv(the_data,
                       v = 6,
                       repeats = 1,
                       strata = LIT_yr,
                       pool = 0.01)
  
  # Extract analysis and assessment data
  cv_out <- mutate(df_vfold,
                   df_cv_trn = map(splits,  analysis),
                   df_cv_tst = map(splits,  assessment)) %>%
    # Fit model to all folds
    mutate(model_fit = map(df_cv_trn,
                           ~fit(lm_model, 
                                m_formula, 
                                data = .x))) %>% 
    # Predict on assessment fold and save observed and predicted
    mutate(model_pred = map2(model_fit,
                             df_cv_tst,
                             ~predict(.x,
                                      new_data = .y,
                                      type = "raw",
                                      opts = list(re.form = NULL)))) %>% 
    mutate(res = map2(df_cv_tst,
                      model_pred,
                      ~data.frame(log_mass_mg = .x$log_mass_mg,
                                  .pred = .y)))
  
  # Save test data and predictions for graphs
  cv_out4graphs <- cv_out %>%
    dplyr::select(df_cv_tst, model_pred) %>%
    unnest(cols = c(df_cv_tst, model_pred))
  
  # Calculate and save metrics
  cv_metrics <- cv_out %>%
    dplyr::select(id, res) %>% 
    tidyr::unnest(res) %>% 
    group_by(id) %>%
    # Get metrics to evaluate model
    metrics(truth = log_mass_mg, estimate = .pred) %>%
    dplyr::select(id,
                  .metric,
                  .estimate) %>%
    pivot_wider(names_from = .metric,
                values_from = .estimate)
  
  # Start of validation to assess option 2
  for (grp in grps) { # Start loop over LIT_yr groups for validation
    
    # Split data leaving one group out at a time as testing data.
    indices <-
      list(analysis   = the_data$.row[the_data$LIT_yr != grp], 
           assessment = the_data$.row[the_data$LIT_yr == grp])
    
    df_split <- make_splits(indices, the_data %>% dplyr::select(-.row))
    df_train <- training(df_split)
    df_test  <- testing(df_split)
    
    # Fit model to all training data for prediction in testing data (grp)
    all_fit <- fit(lm_model, m_formula, data = df_train)
    
    df_test %<>% 
      mutate(model_pred = predict(all_fit,
                                  new_data = df_test,
                                  type = "raw",
                                  opts = list(allow.new.levels = TRUE)))
    
    test_metrics <- metrics(df_test, truth = log_mass_mg, estimate = model_pred) %>%
      mutate(grp_out = grp) %>%
      dplyr::select(grp_out, .metric, .estimate) %>%
      bind_rows(test_metrics, .)
    
    test_data4graphs <- df_test %>%
      bind_rows(test_data4graphs, .)
    
  } # End loop over LIT_yr groups *****
  
  test_metrics %<>%
    pivot_wider(names_from = .metric,
                values_from = .estimate)
  
  return(list(species = spp,
              cv_metrics = cv_metrics,
              cv_out4graphs = cv_out4graphs,
              test_metrics = test_metrics,
              test_data4graphs = test_data4graphs))
  
} # End function definition ******

```

### Function to summarize results of each evaluation

This function takes the list created by the previous function, summarizes and plots results.

```{r summarizePlotFun, echo=FALSE}

summarize_plot_fnc <- function(rslt_list = NULL) {# Start summarize_plot function definition
  
  # Checks for correctness of list, etc. TBA

  # Cross validation summary
  cross_val_metrics <- rslt_list$cv_metrics %>%
    dplyr::select(-id) %>%
    summarize_all(mean) %>%
    mutate(id = "Average") %>%
    bind_rows(rslt_list$cv_metrics, .)
  
  Tbl_cv_cap <- paste("Table of cross validation results for ", rslt_list$species, ". These results represent how well the model performs when all groups have data for d2m.", sep = "")
  
  # Cross validation table
  tbl_cv <-  knitr::kable(cross_val_metrics,
                          digits = c(0, 3, 3, 3),
                          caption = Tbl_cv_cap) %>%
    kable_styling(full_width = FALSE)
  
  # Cross validation figure
  fig_cv <-  ggplot(data = rslt_list$cv_out4graphs,
                    aes(x = model_pred,
                        y = log_mass_mg,
                        color = LIT_yr,
                        groups = LIT_yr)) +
    geom_point() +
    geom_smooth(method = "lm",
                se = TRUE) +
    geom_abline() +
    facet_wrap(~LIT_yr,
               ncol = 2) +
    theme(legend.position = "none") + 
    annotate("text",
             x = 0.90 * max(rslt_list$cv_out4graphs$model_pred, na.rm = TRUE),
             y = 0.15 * max(rslt_list$cv_out4graphs$log_mass_mg, na.rm = TRUE),
             label = rslt_list$species)
  
  
  # Validation summary
  val_metrics <- rslt_list$test_metrics %>%
    dplyr::select(-grp_out) %>%
    summarize_all(mean) %>%
    mutate(grp_out = "Average") %>%
    bind_rows(rslt_list$test_metrics, .)
  
  Tbl_v_cap <- paste("Table of final validation results for ", rslt_list$species, " when data for each refuge is kept out from model creation and used for final testing. This test provides a realistic estimate of the error expected when using data from an observed set of refuge-year combinations to predict seed head mass in a new refuge-year.", sep = "")
  
  # Validation table
  tbl_v <-  knitr::kable(val_metrics,
                         digits = c(0, 3, 3, 3),
                         caption = Tbl_v_cap) %>%
    kable_styling(full_width = FALSE)
  
  # Validation metrics figure
  fig_v <- ggplot(data = rslt_list$test_data4graphs,
                  aes(x = model_pred,
                      y = log_mass_mg,
                      color = LIT_yr,
                      groups = LIT_yr)) +
    geom_point() +
    geom_smooth(method = "lm",
                se = TRUE) +
    geom_abline() +
    facet_wrap(~LIT_yr,
               ncol = 2) +
    theme(legend.position = "none") +
    annotate("text",
             x = 0.90 * max(rslt_list$test_data4graphs$model_pred, na.rm = TRUE),
             y = 0.15 * max(rslt_list$test_data4graphs$log_mass_mg, na.rm = TRUE),
             label = rslt_list$species)
  
  return(list(cv_table = tbl_cv,
              cv_fig = fig_cv,
              v_table = tbl_v,
              v_fig = fig_v))
  
}

```



```{r evalOptions}
# Swam Timothy
st_model_eval <- model_eval_fnc(the_data = st_d2m)
st_rslts <- summarize_plot_fnc(rslt_list = st_model_eval)

# Watergrass
wg_model_eval <- model_eval_fnc(the_data = wg_d2m)
wg_rslts <- summarize_plot_fnc(rslt_list = wg_model_eval)

```

## Swamp Timothy

### Swamp Timothy graphical results

```{r displayGraphsSwampTimothy, echo=FALSE}

print("SWAMP TIMOTHY PREDICTIONS FOR SAMPLED REFUGES")
st_rslts$cv_fig

print("SWAMP TIMOTHY PREDICTIONS FOR NEW REFUGES")
st_rslts$v_fig

```

### Swamp Timothy metrics

```{r displayTablesSwampTimothy, echo=FALSE}

st_rslts$cv_table %>%
  kable_styling(full_width = FALSE,
                position = "float_left")

st_rslts$v_table %>%
  kable_styling(full_width = FALSE,
                position = "left")


```

## Watergrass

### Watergrass graphical results

```{r displayGraphsWatergrass, echo=FALSE}

print("WATERGRASS PREDICTIONS FOR SAMPLED REFUGES")
wg_rslts$cv_fig

print("WATERGRASS PREDICTIONS FOR NEW REFUGES")
wg_rslts$v_fig

```

### Watergrass metrics

```{r displayTablesWatergrass, echo=FALSE}

wg_rslts$cv_table %>%
  kable_styling(full_width = FALSE,
                position = "float_left")

wg_rslts$v_table %>%
  kable_styling(full_width = FALSE,
                position = "left")


```

### How does this compare with the rest of the uncertainty?

Calculate effect of adding this uncertainty to the whole calculation.

### Add model development and transform it into a report.

"Two" questions:

When to sample the sample the same refuge again?
Don't know. Measure all refuges in 3 years. Describe strategy based on results: If it is space, stop. 
How well do models work for unsampled refuge?
How well do models work for sampled refuges?

Title of SOP 5 should reference article titles that have been published about d2m models.
