---
title: "Compare dimensions to mass models over years and refuges"
author: "Emilio A. Laca"
date: "10/29/2021"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}

knitr::opts_chunk$set(echo = TRUE)
library(kableExtra)
library(tidyverse)
library(car)
library(magrittr)
library(readxl)
library(emmeans)
library(lme4)
library(lmerTest)
library(rsample)
library(recipes)
library(tidymodels)
library(LMERConvenienceFunctions)
library(multilevelmod)
library(knitr)

```

## Introduction

WHAP measures linear dimensions of seed heads in the field but attempts to estimate mass of seedheads produced. Dimensions-to-mass (d2m) models are developed and used to transform linear dimensions to estimated mass of seed heads. Then, number of seedheads per unit area and average seed head dimensions are used to estimate mass of seed heads per unit area in each of the abundance strata (low, medium and high) for each species.

Is it necessary to create a different d2m model for each location and year or is it sufficient to create one model that is applicable over refuges and years? Of course, this question can only be answered for past years and observed locations. One can assume that differences (or lack of differences) persist into the future and decide this once and for all or one can continue to test over time. This document is about exploring the answer to the question using data about seed head dimensions and mass collected in several refuges in 2019 and in Modoc in 2021.

## Data

```{r data}

st_2019 <- readRDS("st_all_noO.rds") %>%
  rename_with(~gsub(".", "_", .x, fixed = TRUE)) %>%
  mutate(year = 2019,
         spp = "SwampTimothy",
         f2t_length_mm = NA,
         length_mm = avg_length_mm,
         width_mm = NA,
         mass_mg = mass_per_sh_mg,
         LIT = substr(id, 1, 3)) %>%
  dplyr::select(LIT,
                year,
                spp,
                f2t_length_mm,
                length_mm,
                width_mm,
                mass_mg)

wg_2019 <- readRDS("wgdat.rds") %>%
  rename_with(~gsub(".", "_", .x, fixed = TRUE)) %>%
  mutate(year = 2019,
         spp = "Watergrass",
         f2t_length_mm = F2T_cm *10,
         length_mm = sh_len_cm *10,
         width_mm = NA,
         mass_mg = mass_per_sh_mg,
         LIT = refuge) %>%
  dplyr::select(LIT,
                year,
                spp,
                f2t_length_mm,
                length_mm,
                width_mm,
                mass_mg)

mdc_2021 <- readRDS("sh_dim_MDC2021.rds") %>%
  mutate(year = 2021) %>%
  dplyr::select(LIT,
                year,
                spp,
                f2t_length_mm,
                length_mm,
                width_mm,
                mass_mg)

d2m_data <- bind_rows(st_2019, wg_2019, mdc_2021)

unique(d2m_data$LIT)

# Fix LIT codes
LitKey <- c(CLS = "CLS",
            KRN = "KRN",
            PXL = "PIX",
            SAC = "SAC",
            STL = "SLW",
            PIX = "PIX",
            SL_ = "SLW",
            Colusa = "CLS",
            Sacramento = "SAC",
            StoneLakes = "SLW",
            MDC = "MDC")

d2m_data$LIT <- dplyr::recode(d2m_data$LIT, !!!LitKey)

# Create a grouping variable for LIT and year
d2m_data %<>% mutate(LIT_yr = paste(LIT, year, sep = "_"))

wg_d2m <- d2m_data %>%
  dplyr::filter(spp == "Watergrass") %>%
  mutate(emerged = f2t_length_mm / length_mm) %>%
  arrange(LIT, year)

st_d2m <- d2m_data %>%
  dplyr::filter(spp == "SwampTimothy") %>%
  arrange(LIT, year)

```

## Approach and functions

We have data for Swamp Timothy in six, and for Watergrass in four, different combinations of locations and years. No refuge was sampled more than once to create d2m models. Thus, the variation among groups is mostly spatial. There is no data to estimate the possible variation within a refuge among years.

We should keep in mind that the uncertainty due to these models is NOT incorporated in the final mass estimates in the current protocol. Thus, the effect of choice of approach is mostly invisible.

With these data we have the following options:

1. Ignore the grouping, create a model with all data and use it to make predictions whose uncertainties implicitly include the variation within and between groups. The uncertainty of predictions might be overestimated for observed groups. Assume that this model is good for future groups because the present six groups give a good estimate of the variation among groups in the population.

2. Use grouping to develop a mixed model with all present data and make group-specific predictions for observed groups, and population-level predictions for unobserved (future) groups. This will give more accurate estimates of the uncertainty because it will account for some groups being observed.

For each option we calculate the mean squared prediction error (MSPE) when predicting for observed groups and unobserved groups. The process is as follows:

<br/>

For each group *grp* in the set of LIT_yr groups:

1. Remove group *grp* as the putative "new" group from the data.

2. Perform k-fold cross validation with remaining data.

3. Use group *grp* as testing data and get MSPE.

4. Compare RMSE from cv and MSPE within and between options.

<br/>

### Function to evaluate models

```{r evalFun}
# Model option mld.opt has to be either "fixed" or anything else. Anything else
# results in the "mixed" model.
model_eval_fnc <- function(the_data = NULL,
                           mdl.opt = "fixed"){ # Start f() definition ******
  
  # Make vector of LIT_yr groups to loop over
  grps <- unique(the_data$LIT_yr)
  
  # Create data frames to receive output
  cv_out <- tibble(grp_out = character(),
                   id = character(),
                   .metric =  character(),
                   .estimate = double())
  
  test_out <- tibble(grp_out = character(),
                     .metric =  character(),
                     .estimate = double())
  
  test_data4graphs <- tibble(LIT_yr = character(),
                             mass_mg =  double(),
                             .pred = double())
  
  # Determine what species is being analyzed
  spp <- the_data %>%
    pluck("spp") %>%
    unique()
  
  #Prepare data for manual initial split by LIT_yr
  the_data %<>%
    arrange(LIT_yr) %>%
    mutate(.row = row_number())
  
  # Define model
  if (mdl.opt == "fixed") {
    lm_model <-
      linear_reg() %>%
      set_engine("lm")
  } else {
    lm_model <-
      linear_reg() %>%
      set_engine("lmer")
  }
  
  
  # Define formulae
  frmls <- list(SwampTimothy =
                  list(fixed = log(mass_mg) ~ log(length_mm),
                       mixed = log(mass_mg) ~ log(length_mm) +
                         (0 + log(length_mm) | LIT_yr)),
                Watergrass =
                  list(fixed = log(mass_mg) ~ log(length_mm) +
                         log(f2t_length_mm + 120) +
                         log(emerged + 0.5),
                       mixed = log(mass_mg) ~ log(length_mm) +
                         log(f2t_length_mm + 120) +
                         (0 + log(f2t_length_mm + 120) | LIT_yr) +
                         log(emerged + 0.5) +
                         (0 + log(emerged + 0.5) | LIT_yr)))
  
  
  m_formula <- frmls[[spp]][[mdl.opt]]
  
  for (grp in grps) { # Start loop over LIT_yr groups ******
    
    # Split data leaving one group out at a time as testing data.
    indices <-
      list(analysis   = the_data$.row[the_data$LIT_yr != grp], 
           assessment = the_data$.row[the_data$LIT_yr == grp])
    
    df_split <- make_splits(indices, the_data %>% dplyr::select(-.row))
    df_train <- training(df_split)
    df_test  <- testing(df_split)
    df_vfold <- vfold_cv(df_train,
                         v = 5,
                         repeats = 3,
                         strata = LIT_yr)
    

    # Extract analysis/training and assessment/testing data
    cv_out <- mutate(df_vfold,
                     df_cv_trn = map(splits,  analysis),
                     df_cv_tst = map(splits,  assessment)) %>%
      # # prep, juice, bake data for fitting
      # mutate(
      #   recipe =  map(df_cv_trn, ~prep(df_rec, training = .x)),
      #   df_cv_trn =  map(recipe, juice),
      #   df_cv_tst = map2(recipe, df_cv_tst, ~bake(.x, new_data = .y))) %>% 
      # Fit model to all folds
      mutate(
        model_fit  = map(df_cv_trn, ~fit(lm_model, m_formula, data = .x))) %>% 
      # Predict on assessment fold and save observed and predicted
      mutate(
        model_pred = map2(model_fit, df_cv_tst, ~predict(.x, new_data = .y))) %>% 
      mutate(res = map2(df_cv_tst, model_pred, ~data.frame(log_mass_mg = log(.x$mass_mg),
                                                           .pred = .y$.pred))) %>% 
      dplyr::select(id, res) %>% 
      tidyr::unnest(res) %>% 
      group_by(id) %>%
      # Get metrics to evaluate model
      metrics(truth = log_mass_mg, estimate = .pred) %>%
      mutate(grp_out = grp) %>%
      dplyr::select(grp_out,
                    id, .metric,
                    .estimate) %>%
      bind_rows(cv_out, .)
    
    # Fit model to all training data for prediction in testing data (grp)
    # recipe <- df_train %>% prep(df_rec, training = .)
    # 
    # df_train_juiced <- recipe %>%
    #   juice()
    # 
    # df_test_baked <- recipe %>%
    #   bake(new_data = df_test)
    
    all_fit <- fit(lm_model, m_formula, data = df_train)
    
    df_test %<>% 
      mutate(.pred = if (mdl.opt == "fixed") {
               predict(all_fit, new_data = df_test)$.pred
        } else {
          predict(all_fit,
                  new_data = df_test,
                  type = "raw",
                  opts = list(allow.new.levels = TRUE))
        },
             logmass = log(mass_mg))
    
    test_data4graphs <- df_test %>%
      dplyr::select(LIT_yr,
                    mass_mg,
                    .pred) %>%
      bind_rows(test_data4graphs, .)
    
    test_out <- metrics(df_test, truth = logmass, estimate = .pred) %>%
      mutate(grp_out = grp) %>%
      dplyr::select(grp_out, .metric, .estimate) %>%
      bind_rows(test_out, .)
    
  } # End loop over LIT_yr groups *****
  
  return(list(species = spp,
              model = mdl.opt,
              cross_val = cv_out,
              test = test_out,
              d4graphs = test_data4graphs))
  
} # End function definition ******

```

### Function to summarize results of each evaulation

This function takes the list created by the previous function, summarizes and plots results.

```{r summarizePLot}

summarize_plot_fnc <- function(rslt_list = NULL) {# Start summarize_plot function definition

# Checks for correctness of list, etc. TBA
  
  # Cross validation summary
  cross_val_metrics <- rslt_list$cross_val %>%
    group_by(grp_out, .metric) %>%
    summarise(.estimate = mean(.estimate),
              .groups = "drop") %>%
    pivot_wider(names_from = .metric,
                values_from = .estimate)
  
  Tbl_cv_cap <- paste("Table of cross validation results for ", rslt_list$species, " when data for each refuge is kept out for final testing and not used at all in the cv. A ", rslt_list$model, " model was used. Keep in mind that the Rsq does not consider deviations from the 1:1 line but from a fitted line between observed and predicted values.", sep = "")
  
  # Cross validation table
  tbl_cv <-  knitr::kable(cross_val_metrics,
                       digits = c(0, 3, 3, 3),
                       caption = Tbl_cv_cap) %>%
    kable_styling(full_width = FALSE)
  
  # Cross validation metrics figure
  fig_cv <-  ggplot(data = cross_val_metrics, aes(x = rmse, y = rsq)) +
    geom_point(color = "blue", size = 3) + 
    geom_label(aes(label = paste("NOT ", grp_out, sep = "")),
               hjust = 0.0,
               vjust = 0.0) +
    xlab("Root mean squared error in cross-validation") +
    ylab("R-squared in cross-validation") +
    xlim(0, 1.4) +
    ylim(0, 1) +
    annotate("text", x = 1.2, y = 0.96, label = rslt_list$species) +
    annotate("text", x = 1.2, y = 0.92, label = rslt_list$model)
  
  
  # Validation summary
    val_metrics <- rslt_list$test %>%
    pivot_wider(names_from = .metric,
                values_from = .estimate)
  
  Tbl_v_cap <- paste("Table of final validation results for ", rslt_list$species, " when data for each refuge is kept out from model creation and used for final testing. A ", rslt_list$model, " model was used. This test provides a realistic estimate of the error expected when using data from an observed set of refuge-year combinations to predict seed head mass in a new combination. Keep in mind that the Rsq does not consider deviations from the 1:1 line but from a fitted line between observed and predicted values.", sep = "")
  
  # Validation table
  tbl_v <-  knitr::kable(val_metrics,
                       digits = c(0, 3, 3, 3),
                       caption = Tbl_v_cap) %>%
    kable_styling(full_width = FALSE)
  
  # Validation metrics figure
  fig_v <-  ggplot(data = val_metrics, aes(x = rmse, y = rsq)) +
    geom_point(color = "blue", size = 3) + 
    geom_label(aes(label = paste("NOT ", grp_out, sep = "")),
               hjust = 0.0,
               vjust = 0.0) +
    xlab("Root mean squared error in cross-validation") +
    ylab("R-squared in cross-validation") +
    xlim(0, 1.4) +
    ylim(0, 1)
    annotate("text", x = 1.2, y = 0.96, label = rslt_list$species) +
    annotate("text", x = 1.2, y = 0.92, label = rslt_list$model)

    # Validation obs. vs pred. figure
    fig_v2 <- ggplot(data = d4graphs,
                     aes(x = exp(.pred),
                         y = mass_mg,
                         groups = LIT_yr,
                         color = LIT_yr))

return(list(cv_table = tbl_cv,
            cv_fig = fig_cv,
            v_table = tbl_v,
            v_fig = fig_v))
    
}

```


## Models for Swamp Timothy

### Data exploration

```{r st_plotData}

scatterplot(mass_mg ~ length_mm | LIT_yr,
            smooth = FALSE,
            log = "xy",
            data = st_d2m)

```

There is some indication that SAC had heavier seed heads than the rest of the groups for any given seed head length.

### Test of effects of group (LIT_yr)

```{r st_modelFixedGroups}

# Get optimal lambda
st_lamb1 <- powerTransform(lm(mass_mg ~ log(length_mm),
                              data = st_d2m)) %>%
  coef(round = TRUE) %>%
  unname()

st_lamb1 # = 0, log transform

st_d2m_m1 <- lm(log(mass_mg) ~ log(length_mm) * LIT_yr,
                data = st_d2m)

opar <- par(mfrow = c(2,2)); plot(st_d2m_m1); par(opar)

anova(st_d2m_m1)

summary(st_d2m_m1)

# Remove interaction

st_d2m_m2 <- lm(log(mass_mg) ~ log(length_mm) + LIT_yr,
                data = st_d2m)

```

The model indicates that there are no difference in slopes, but that there are some differences in heights of the lines. The differences in height at average seed head length can be tested with emmeans.


```{r st_compareIntcpt}

avg_length <- mean(st_d2m$length_mm)

yhat_at_xbar = emmeans(st_d2m_m2,
                       "LIT_yr",
                       at = list(length_mm = avg_length))

pairs(yhat_at_xbar)

```

As suggested by the graphical analysis, the test shows that SAC has heavier timothy seed heads for a given length than the other refuges. There are no other significant differences.

### Random-effect approach Swamp Timothy

Each group of observations within a refuge and year can be considered a random draw of all possible groups. This creates a basis to estimate seed head mass for unobserved groups. In the case of the observed groups we make predictions that include the random effect of the group and exclude the uncertainty due to group. In the case of unobserved new groups, predictions are made at the population level and uncertainty includes a variance component due to deviations of groups about the population.

In this approach, the question of whether a new refuge or year need to be sampled can be addressed by exploring the magnitude of group-level variance.

```{r st_modelRandomGroups}

st_d2m_m3 <- lmer(log(mass_mg) ~ log(length_mm) + (log(length_mm)|LIT_yr),
                  data = st_d2m)

summary(st_d2m_m3)

ranef(st_d2m_m3)

opar <- par(mfrow = c(2,2)); plot(st_d2m_m3); par(opar)

st_d2m_m4 <- update(st_d2m_m3, . ~ log(length_mm) + (0 + log(length_mm)|LIT_yr))

st_d2m_m5 <- update(st_d2m_m3, . ~ log(length_mm) + (1|LIT_yr))

anova(st_d2m_m3, st_d2m_m4)

anova(st_d2m_m4, st_d2m_m5) # Model 4 is slightly better.

```

Contrary to the fixed-effects approach, the mixed model indicates that there is more variation due to slopes than to intercepts. However, there are no statistically significant differences between models.

```{r st_plotRanef}

rnf_st_d2m_m3 <- ranef(st_d2m_m3) %>%
  as.data.frame() %>%
  pivot_wider(id_cols = "grp",
              values_from = c(condval, condsd),
              names_from = "term") %>%
  rename(cond_Incpt_Ranef = `condval_(Intercept)`,
         cond_slope_Ranef = `condval_log(length_mm)`,
         cond_Incpt_sd = `condsd_(Intercept)`,
         cond_slope_sd = `condsd_log(length_mm)`)

ggplot(data = rnf_st_d2m_m3,
       aes(x = cond_Incpt_Ranef,
           y = cond_slope_Ranef)) +
  geom_point(color = "blue", size = 3) + 
  geom_errorbar(aes(ymin = cond_slope_Ranef - cond_slope_sd,
                    ymax = cond_slope_Ranef + cond_slope_sd),
                color = "blue") +
  geom_errorbarh(aes(xmin = cond_Incpt_Ranef - cond_Incpt_sd,
                     xmax = cond_Incpt_Ranef + cond_Incpt_sd),
                 color = "orange") +
  geom_label(aes(label = grp), hjust = 0.5, vjust = 0.3) +
  xlab("Random effect on intercept") +
  ylab("Random effect on slope")

```


Population-level predictions and estimates are identical in both approaches.

Models should also be compared in terms of the impact on estimated mass of seed heads in sub units and refuges. Statistically significant differences do not necessarily translate into differences of practical significance, particularly when there are other sources of larger variance in the estimates, and considering that it is more time costly to create new models for each location and year.

### Comparison of options for  Swamp Timoty models

```{r evalSwampTimothy}

st_model_eval_fxd <- model_eval_fnc(the_data = st_d2m, mdl.opt = "fixed")

st_model_eval_mxd <- model_eval_fnc(the_data = st_d2m, mdl.opt = "mixed")

# Make tables and graphs of results saved in st_model_eval

vis_st_fxd <-  summarize_plot_fnc(rslt_list = st_model_eval_fxd)

vis_st_mxd <-  summarize_plot_fnc(rslt_list = st_model_eval_mxd)

```

Model evaluation calculates residual mean squared error of predictions (MSPE or rmse), R-squared and mean absolute deviations for each replicate of the cross validation sets and for the test data, which are the group or refuge-year combination left out of the training data.

## Models for Watergrass

### Data Exploration

```{r wg_plotData}

pairs(wg_d2m[ c("length_mm", "f2t_length_mm", "emerged")])

biplot(prcomp(wg_d2m[c("length_mm", "f2t_length_mm", "emerged")], scale. = TRUE))

scatterplot(mass_mg ~ length_mm | LIT_yr,
            smooth = FALSE,
            log = "xy",
            data = wg_d2m)

scatterplot(mass_mg ~ log(f2t_length_mm + 120) | LIT_yr,
            smooth = FALSE,
            log = "y",
            data = wg_d2m)

scatterplot(mass_mg ~ log(emerged + 0.5) | LIT_yr,
            smooth = FALSE,
            log = "y",
            data = wg_d2m)


```

Predictors have some, but not excessive, collinearity. I surmise that `length_mm` is the size dimension, whereas the combination of `f2t_length_mm` and `emerged` is a measure of phenological development.

**Question: should predictions be done for the measured degree of seed head development or for a fixed level selected to correct for degree of development when seed heads were measured?**

### Test of effects of group (LIT_yr)

```{r wg_modelFixedGroups}

# Get optimal lambda
wg_lamb1 <- powerTransform(lm(mass_mg ~ log(length_mm) +
                                log(f2t_length_mm + 120) +
                                log(emerged + 1),
                              data = wg_d2m)) %>%
  coef(round = TRUE) %>%
  unname()

wg_lamb1 # = 0, log transform

wg_d2m_m1 <- lm(log(mass_mg) ~ (log(length_mm) +
                                  log(f2t_length_mm + 120) +
                                  log(emerged + 1)) * LIT_yr,
                data = wg_d2m)

opar <- par(mfrow = c(2,2)); plot(wg_d2m_m1); par(opar)

anova(wg_d2m_m1)
Anova(wg_d2m_m1)

summary(wg_d2m_m1)

```

In this case there is a clear effect of group on the relationship between seed head mass and dimensions. Modoc 2021 is the group that differs the most from the rest. The specific differences are studied below when using a mixed model approach, which is more appropriate for testing the need to have random effects for groups on slopes and intercept.

```{r wg_estimates}

avg_length <- mean(wg_d2m$length_mm)
avg_f2t <- mean(wg_d2m$f2t_length_mm)
avg_emerged <- mean(wg_d2m$emerged)

yhat_at_xbar = emmeans(wg_d2m_m1,
                       "LIT_yr",
                       at = list(length_mm = avg_length,
                                 f2t_length_mm = avg_f2t,
                                 emerged = avg_emerged)) %>%
  as_tibble() %>%
  mutate(mass_mg = exp(emmean),
         lower.CL_mg = exp(lower.CL),
         upper.CL_mg = exp(upper.CL),
         coeff.var2 = SE/sqrt(SE^2 + emmean^2))

```

The coefficient of variation (second order CV, Tarald O. Kvålseth (2017) Coefficient of variation: the second-order alternative,
Journal of Applied Statistics, 44:3, 402-415, DOI: 10.1080/02664763.2016.1174195) is very small, indicating high precision in the estimates.

### Random-effect approach Watergrass

```{r wg_modelRandomGroups}

wg_d2m_m2 <- lmer(log(mass_mg) ~ log(length_mm) +
                    (0 + log(length_mm) | LIT_yr) +
                    log(f2t_length_mm + 120) +
                    (0 + log(f2t_length_mm + 120) | LIT_yr) +
                    log(emerged + 0.5) +
                    (0 + log(emerged + 0.5) | LIT_yr) +
                    (1 | LIT_yr),
                  data = wg_d2m)

summary(wg_d2m_m2)

wg_d2m_m3 <- update(wg_d2m_m2, . ~ . - (0 + log(length_mm) | LIT_yr))

summary(wg_d2m_m3)

wg_d2m_m4 <- update(wg_d2m_m3, . ~ . -(1 | LIT_yr)) 

summary(wg_d2m_m4)

anova(wg_d2m_m2, wg_d2m_m3)

anova(wg_d2m_m2, wg_d2m_m4) # wg_d2m_m4 is better

wg_d2m_m5 <- update(wg_d2m_m4,
                    . ~ . -log(f2t_length_mm + 120) -
                      (0 + log(emerged + 0.5) | LIT_yr)) 

anova(wg_d2m_m5, wg_d2m_m2)

```

### Comparison of options for Watergrass models

```{r evalWatergrass, warning=FALSE}

wg_model_eval1 <- model_eval_fnc(the_data = wg_d2m, mdl.opt = "fixed")

wg_model_eval2 <- model_eval_fnc(the_data = wg_d2m, mdl.opt = "mixed")

# Make tables and graphs of results saved in st_model_eval

```


### Check results for MDC

Results are correct. The rsq metric of yardstick is calculated as the squared correlation between predicted and observed values, which ignores the bias in predictions.

```{r checkMDC, eval=FALSE, include=FALSE}

mdc_train <- wg_d2m %>%
  dplyr::filter(LIT_yr != "MDC_2021")

mdc_m1 <- lm(log(mass_mg) ~ (log(length_mm) +
                                  log(f2t_length_mm + 120) +
                                  log(emerged + 0.5)),
                data = mdc_train)

opar <- par(mfrow = c(2,2)); plot(mdc_m1); par(opar)


mdc_test <- wg_d2m %>%
  dplyr::filter(LIT_yr == "MDC_2021")

mdc_test %<>%
  mutate(pred = exp(predict(mdc_m1, newdata = .)))

with(mdc_test, plot(mass_mg ~ pred))
abline(0, 1)

with(mdc_test, plot(log(mass_mg) ~ log(pred)))
abline(0, 1)

cor(log(mdc_test$mass_mg), log(mdc_test$pred))^2

sd(log(mdc_test$mass_mg) - log(mdc_test$pred))

((log(mdc_test$mass_mg) - log(mdc_test$pred))^2/length(mdc_test$mass_mg)) %>%
  sum() %>%
  sqrt()


```


